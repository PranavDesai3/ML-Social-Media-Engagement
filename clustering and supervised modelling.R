#------- install necessary packages and load libraries ------------------------#
library(ggplot2)
library(Hmisc)
library(tidyverse)
library(dplyr)
library(caret)
library(factoextra)
library(cluster)
library(reshape2)
library(skimr)
library(mlr3)
library(mlr3verse)
library(kknn)
library(mlr3learners)
#------------------------------------------------------------------------------#

#--------------------- Data Evaluation ----------------------------------------#
#reading the data from file
data <- read.csv('Data_DMML.csv')

Hmisc::describe(data)
#We can observe from the summary generated by above function that there are no missing values in any of the attributes.
#MeanWordCount attributes shows a lot of high values which are very well seperated from each other (1234    1271.33 1828    2008    3118) in the highest category which could indicate that some users might be more more verbal with their posts. It could also mean that the user might have less number of posts but uploaded those with a long captions that inceased the MeanWordCount. We could potentially remove these outliers when clustering. 
#PercentQuestions and PercentURLs - from the definition of it, it looks like this attributes provides little to no information that is relevant to the users activity. We might think of ignoring these during clustering

cor_mat <- cor(data)

corrplot::corrplot(cor_mat, method = 'ellipse', type = 'upper')
#There is high correlation between InDegree, OutDegree and TotalPosts attributes.
#The high correlation between these 3 attributes can pose an issue when forming the clusters
#Hence we will combine these 3 into one single attribute called 'SocialMediaActivityIndex'.
#Since all 3 parameters are of almost equal importance we cannot give them different individual weightage.
#We will assign 33% weightage to each InDegree and OutDegree attribute and the remaining 34% to the TotalPost attribute.

# Calculate the 99th percentile for the specified variables
p99_WordCount <- quantile(data$MeanWordCount, 0.99)
p99_PostsPerThread <- quantile(data$MeanPostsPerThread, 0.99)
p99_PostsPerSubForum <- quantile(data$MeanPostsPerSubForum, 0.99)

#We do not remove these in the final report###########################
# Remove rows based on the 99th percentile threshold for all specified attributes
data <- data %>%
  filter(MeanWordCount <= p99_WordCount,
         MeanPostsPerThread <= p99_PostsPerThread,
         MeanPostsPerSubForum <= p99_PostsPerSubForum)

Hmisc::describe(data)
#------------------------------------------------------------------------------#

#---------------------- Data Transformation -----------------------------------#
#Creating combined feature
data_for_clustering <- data %>%
  mutate(NetworkActivityIndex = (0.33 * data[, "InDegree"] + 
                                    0.33 * data[, "OutDegree"] + 
                                    0.34 * data[, "TotalPosts"])) %>%
  select(-InDegree, -OutDegree, -TotalPosts, -ID)

Hmisc::describe(data_for_clustering)
cor_mat <- cor(data_for_clustering)
corrplot::corrplot(cor_mat, method = 'ellipse', type = 'upper')
#------------------------------------------------------------------------------#

#---------- Standardising data for clustering ---------------------------------#
# Prepare the data for scaling (excluding the ID column)

scaled_data <- scale(data_for_clustering)
Hmisc::describe(scaled_data)

#------------------------------------------------------------------------------#

#-------------- PCA -----------------------------------------------------------#
# Perform PCA
pca_result <- prcomp(scaled_data, center = TRUE, scale. = FALSE)

# Determine the number of principal components to retain
# This can be done using a scree plot
fviz_eig(pca_result)

# Assuming we decide to retain '4' components, use them for K-means clustering
# Extract the principal components
data_pca <- as.data.frame(pca_result$x[, 1:2])
#------------------ Finding Optimal number of clusters ------------------------#
# Determine the optimal number of clusters using the elbow method
set.seed(123) # For reproducibility

fviz_nbclust(data_pca, kmeans, method = "wss")
fviz_nbclust(data_pca, kmeans, method = "silhouette")

#------------------------------------------------------------------------------#

#-------------------- Running K-Means -----------------------------------------#
# Run K-means clustering
set.seed(123) # For reproducibility

kmeans_results <- kmeans(data_pca, centers =3, nstart = 50, iter.max = 100) #WSS starts to level off, which could be around 3 clusters

# Check the results
print(kmeans_results)
print(kmeans_results$size)

# Visualize the clusters for 3 cluster scenario
fviz_cluster(kmeans_results, geom = "point", data = data_pca)

#Based on what you choose to keep, add the cluster result of that to the data.

# Add the cluster assignments to your dataframe
data$cluster <- kmeans_results$cluster
data_for_clustering$cluster <- kmeans_results$cluster

# Convert cluster assignments to a factor for modeling
data$cluster <- as.factor(data$cluster)
data_for_clustering$cluster <- as.factor(data_for_clustering$cluster)

#----------------- Result Evaluation ------------------------------------------#

sil_scores <- silhouette(kmeans_results$cluster, dist(data_pca))
mean_silhouette_width <- mean(sil_scores[, "sil_width"])
print(paste("Average silhouette width:", mean_silhouette_width))
fviz_silhouette(sil_scores)
# Summary of silhouette analysis

# Objects with negative silhouette
neg_sil_index <- which(sil_scores[, 'sil_width'] < 0)
sil_scores[neg_sil_index, , drop = FALSE]

# Within-Cluster Sum of Squares by Cluster
wss_by_cluster <- kmeans_results$withinss
print(paste("Within-cluster sum of squares by cluster:", toString(wss_by_cluster)))

# Total Within-Cluster Sum of Squares
total_wss <- sum(kmeans_results$withinss)
print(paste("Total within-cluster sum of squares:", total_wss))

# Between-Cluster Sum of Squares
total_ss <- sum((data_pca - apply(data_pca, 2, mean))^2)
bss <- total_ss - total_wss
print(paste("Between-cluster sum of squares:", bss))

# BSS/TSS ratio
bss_tss_ratio <- bss / total_ss
print(paste("Between-SS / Total-SS:", bss_tss_ratio))

# Average Within-Cluster Distance
avg_within_cluster_dist <- kmeans_results$tot.withinss / length(kmeans_results$cluster)
print(paste("Average within-cluster distance:", avg_within_cluster_dist))

#------------------------------------------------------------------------------#

#For cluster interpretation purpose
write_xlsx(data_for_clustering, "C:/Users/prana/OneDrive/Desktop/test.xlsx")

#------------------------------------------------------------------------------#

aggregate(data = data_for_clustering, NetworkActivityIndex ~ cluster, mean)

#------------------------------------------------------------------------------#

#------------------------------------------------------------------------------#
#Visualize clusters within convex space
fviz_cluster(kmeans_results, data = data_pca, 
             ellipse.type = "convex", palette = "jco", 
             ggtheme = theme_minimal())

#------------------------------------------------------------------------------#
ggplot(data_for_clustering, aes(NetworkActivityIndex,AccountAge)) +
  geom_point(aes(color = as.factor(cluster)))+
  geom_smooth(method = "lm")+
  labs(y = "Age of the user acount in months", x="User's Social Media Activity", title = "Social Media Activity VS Age of User Account", color = "cluster")

#------------------------------------------------------------------------------#


#--------------------- MLR3: KNN ----------------------------------------------#
 
#lets create the task using the data we used for clustering, and setting our target as cluster
task <- as_task_classif(data,
                       target='cluster') 

task

set.seed(123) 

train_set = sample(task$row_ids, 0.7 * task$nrow)
test_set = setdiff(task$row_ids, train_set)

#lets choose our learner i.e. knn
learner = lrn("classif.kknn")
learner 


#by default it picks k=7, we can change this in the learner
#we tune the value of k to check what results we get for our model
learner = lrn("classif.kknn", k=10)
learner = lrn("classif.kknn", k=1)
learner = lrn("classif.kknn", k=5)
learner = lrn("classif.kknn", k=14)
learner 

# training
learner$train(task, row_ids = train_set)

#lets use the model to predict the training set
pred_train = learner$predict(task, row_ids=train_set)  
pred_train$confusion #print the conf matrix

measures = msrs(c('classif.acc', 'classif.ce'))
pred_train$score(measures)  

# use the model to predict on the TEST dataset 
pred_test = learner$predict(task, row_ids=test_set)
pred_test$confusion #get the confusion matrix 

measures = msrs(c('classif.acc', 'classif.ce'))
pred_test$score(measures)

